{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Neil Garrett, June 2018 **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Start up commands/load relevant functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Precompiling CSV [336ed68f-0bac-5ca0-87d4-7b16caf5d00b]\n",
      "└ @ Base loading.jl:1187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      From worker 4:\t┌ Warning: Module Tables with build ID 63066342871565 is missing from the cache.\n",
      "      From worker 4:\t│ This may mean Tables [bd369af6-aec1-5ad0-b16a-f7cc5008161c] does not support precompilation but is imported by a module that does.\n",
      "      From worker 4:\t└ @ Base loading.jl:941\n",
      "      From worker 3:\t┌ Warning: Module Tables with build ID 63066342871565 is missing from the cache.\n",
      "      From worker 3:\t│ This may mean Tables [bd369af6-aec1-5ad0-b16a-f7cc5008161c] does not support precompilation but is imported by a module that does.\n",
      "      From worker 3:\t└ @ Base loading.jl:941\n",
      "      From worker 5:\t┌ Warning: Module Tables with build ID 63066342871565 is missing from the cache.\n",
      "      From worker 5:\t│ This may mean Tables [bd369af6-aec1-5ad0-b16a-f7cc5008161c] does not support precompilation but is imported by a module that does.\n",
      "      From worker 5:\t└ @ Base loading.jl:941\n",
      "      From worker 2:\t┌ Warning: Module Tables with build ID 63066342871565 is missing from the cache.\n",
      "      From worker 2:\t│ This may mean Tables [bd369af6-aec1-5ad0-b16a-f7cc5008161c] does not support precompilation but is imported by a module that does.\n",
      "      From worker 2:\t└ @ Base loading.jl:941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Module Tables with build ID 63066342871565 is missing from the cache.\n",
      "│ This may mean Tables [bd369af6-aec1-5ad0-b16a-f7cc5008161c] does not support precompilation but is imported by a module that does.\n",
      "└ @ Base loading.jl:941\n"
     ]
    }
   ],
   "source": [
    "# load required libraries\n",
    "using Distributed\n",
    "\n",
    "# # set everything up\n",
    "parallel = true # Run on multiple CPUs. \n",
    "\n",
    "# this activates the multiprocessing threads\n",
    "if (parallel)\n",
    "\t# only run this once\n",
    "    addprocs(4)\n",
    "end\n",
    "\n",
    "# load required libraries\n",
    "\n",
    "@everywhere using DataFrames\n",
    "#using DataArrays\n",
    "@everywhere using ForwardDiff\n",
    "@everywhere using PyCall\n",
    "@everywhere using Distributions\n",
    "@everywhere using PyPlot\n",
    "@everywhere using CSV\n",
    "@everywhere using SpecialFunctions\n",
    "@everywhere using SharedArrays\n",
    "@everywhere using LinearAlgebra\n",
    "\n",
    "@everywhere PyCall.@pyimport scipy.optimize as so\n",
    "\n",
    "# this is the code for the actual fitting routines\n",
    "@everywhere include(\"em.jl\")\n",
    "@everywhere include(\"common.jl\")\n",
    "@everywhere include(\"likfuns.jl\")\n",
    "\n",
    "# this is generates starting matricies for betas, sigmas etc to feed into model\n",
    "@everywhere include(\"genVars.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Data read and process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Read in trial by trial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#read in csv file of the data\n",
    "#trial by trial data: note will include force trials and missed responses\n",
    "df = readtable(\"/Users/neil/GitHubRepo/Projects/PreySelection/v103/data/trialdata_103_processed.csv\")\n",
    "\n",
    "#display header\n",
    "head(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Append data with the column \"sub\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#this is just a replica of the existing column sub_no but think em looks for \"sub\" specifically\n",
    "df[:sub] = df[:subj];\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Get rid of excluded subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = df[df[:exclude].==0,:];\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Convert approach avoid to 2s and 1s , missed as 0. Then convert to integers (necessary to use as an index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#convert approach_avoid to 1s (avoid) and 2s (approach)\n",
    "df[df[:approach_avoid].==1,:approach_avoid] = 2\n",
    "df[df[:approach_avoid].==-1,:approach_avoid] = 1\n",
    "\n",
    "index_NaN = find(isnan.(df[:approach_avoid]))\n",
    "df[index_NaN, :approach_avoid] = 0\n",
    "\n",
    "df[:approach_avoid] = convert(Vector{Integer}, df[:approach_avoid])\n",
    "\n",
    "head(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Read in summary stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "summary_stats = readtable(\"/Users/neil/GitHubRepo/Projects/PreySelection/v103/data/subdata_103.csv\")\n",
    "head(summary_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Get rid of excluded subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "summary_stats = summary_stats[summary_stats[:exclude].==0,:];\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Get rid of Mturk ID etc (first 3 columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "summary_stats = summary_stats[:,4:end];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asymmetry Model\n",
    "\n",
    "This model comprises: \n",
    "\n",
    "1. An intercept which reflects degree of bias to reject.\n",
    "\n",
    "2. A beta (termperature parameter) which controls sensitivity to the difference between the options (0 = pick 50/50. Higher it is, the more sensative subs are tothe different options (more step functionesque). <br>\n",
    "\n",
    "3. Two learning rates: one for appetative component (reward), one for aversive (delay)\n",
    "\n",
    "Uses Q learned average to predict choice\n",
    "\n",
    "Initalise Qaverage in model at the arithmetic average over all subs over both sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@everywhere function model_asymmetry(params, data)\n",
    "     \n",
    "    #model parameters\n",
    "    intercept = params[1]\n",
    "    beta = params[2]\n",
    "    lr_reward = 0.5 .+ 0.5.*erf(params[3]/sqrt(2))\n",
    "    lr_delay = 0.5 .+ 0.5.*erf(params[4]/sqrt(2))\n",
    "    initial = 0.5 .+ 0.5.*erf(params[5]/sqrt(2))\n",
    "    \n",
    "    #initalise various variables\n",
    "    delay_sum = zeros(typeof(beta),1)\n",
    "    reward_sum = zeros(typeof(beta),1)\n",
    "    Q_arithmetic = zeros(typeof(beta),1) # stores actual (arithmetic) average reward rate\n",
    "    opp_cost_arithmetic = zeros(typeof(beta),1) # stores actual (arithmetic) opp cost\n",
    "    Q_estimate = zeros(typeof(beta),1) .+ initial # stores estimated global reward rate\n",
    "    opp_cost_estimate = zeros(typeof(beta),1) # stores estimated opp cost\n",
    "    \n",
    "    Qd = zeros(typeof(beta),2) # decision variable; 1st element is the opp cost of accepting (or value of rejecting), 2nd element is just the reward of the option (value of accepting)\n",
    "\n",
    "    lik = 0 #likelihood\n",
    "\n",
    "    #these store new trial by trial values (e.g. Q estimate on each trial etc.)\n",
    "    reward_sum_store = [];\n",
    "    delay_sum_store = [];\n",
    "    Q_arithmetic_store = [];\n",
    "    opp_cost_arithmetic_store  = []; \n",
    "    Q_estimate_store = [];\n",
    "    opp_cost_estimate_store = [];\n",
    "\n",
    "    #extract various variables from the dataframe\n",
    "    reward = data[:reward_percent]\n",
    "    delay = data[:delay_s]\n",
    "    force = data[:force_trial]  \n",
    "    missed = data[:missed] #missed responses \n",
    "    c = data[:approach_avoid] #choice\n",
    "        \n",
    "    for i = 1:length(c)\n",
    "        \n",
    "            # 2 seconds without reward on each trial regadless of accept/reject\n",
    "            delay_sum .+= 2;\n",
    "         \n",
    "            # calculate current (arithmetic) reward per second from number of seconds elapsed and reward accured\n",
    "            Q_arithmetic = reward_sum./delay_sum\n",
    "            opp_cost_arithmetic = Q_arithmetic*delay[i]\n",
    "           \n",
    "            # decrease estimate of global reward rate for encounter time (2seconds)\n",
    "            Q_estimate = (1-lr_delay) * Q_estimate .+ 0\n",
    "            Q_estimate = (1-lr_delay) * Q_estimate .+ 0\n",
    "        \n",
    "            #calculate estimate of opportunity cost given estimate of reward rate and delay incurred by option \n",
    "            opp_cost_estimate = Q_estimate*delay[i]\n",
    "        \n",
    "            #add trial by trial values \n",
    "            append!(reward_sum_store, reward_sum)\n",
    "            append!(delay_sum_store, delay_sum)\n",
    "            append!(Q_arithmetic_store, Q_arithmetic)\n",
    "            append!(opp_cost_arithmetic_store, opp_cost_arithmetic)\n",
    "            append!(Q_estimate_store, Q_estimate)\n",
    "            append!(opp_cost_estimate_store, opp_cost_estimate)\n",
    "        \n",
    "            # if not a force trial predict choice based on current values\n",
    "            if ((force[i]<1) & (missed[i]<1))\n",
    "                        \n",
    "                # decision variable - the estimate of opportunity cost (\"reward\" of rejecting) versus \n",
    "                # reward of the current option (if accepted)\n",
    "                Qd = [intercept, 0] .+ [beta.*opp_cost_estimate[1], beta.*reward[i]]\n",
    "\n",
    "                # increment likelihood\n",
    "                lik += Qd[c[i]] - log(sum(exp.(Qd)))\n",
    "            \n",
    "            end\n",
    "            \n",
    "            #incur 8second time out for missed response\n",
    "            if (missed[i]==1)\n",
    "                \n",
    "                delay_sum .+= 8\n",
    "            \n",
    "                for j = 1:8\n",
    "                \n",
    "                     Q_estimate = (1-lr_delay) * Q_estimate .+ 0\n",
    "\n",
    "                end\n",
    "            \n",
    "            end\n",
    "        \n",
    "            # regardless of whether a force trial or not, \n",
    "            # if accept the option, Q_estimate updates and there is a delay incurred\n",
    "            if ((c[i] == 2) & (missed[i]==0))\n",
    "                \n",
    "                delay_sum .+= delay[i]\n",
    "                reward_sum .+= reward[i]\n",
    "            \n",
    "                for j = 1:delay[i]\n",
    "                \n",
    "                    Q_estimate = (1-lr_delay) * Q_estimate .+ 0\n",
    "                \n",
    "                end\n",
    "            \n",
    "                    Q_estimate = (1-lr_reward) * Q_estimate .+ lr_reward*reward[i]\n",
    "                \n",
    "            end\n",
    "    \n",
    "    end\n",
    "    \n",
    "    # compile trial by trial values here\n",
    "    trial_data = DataFrame(reward_sum = reward_sum_store,\n",
    "            delay_sum = delay_sum_store,\n",
    "            Q_arithmetic = Q_arithmetic_store,\n",
    "            opp_cost_arithmetic = opp_cost_arithmetic_store,\n",
    "            Q_estimate = Q_estimate_store,\n",
    "            opp_cost_estimate = opp_cost_estimate_store)\n",
    "    \n",
    "    # here if running em you can only return the likelihood\n",
    "    return -lik\n",
    "    \n",
    "    # but if you run in order to extract trials, subs etc then want to return this\n",
    "    #return (-lik, trial_data)\n",
    "    \n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter optimisiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model for one subject\n",
    "\n",
    "aids debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameter structures\n",
    "(df, subs, X, betas, sigma) = genVars(df, 5);\n",
    "\n",
    "# run model for sub 1\n",
    "model_asymmetry(betas,df[df[:sub].==subs[1],:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run em to get best fit parameters for each subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialized parameter structures (again)\n",
    "# note that some of the variables (e.g. betas, sigma) are entered and returned by em function \n",
    "(df, subs, X, betas, sigma) = genVars(df, 4);\n",
    "\n",
    "# run for full learner\n",
    "# x contains the parameters for each subject (note not the same as variable X)\n",
    "# l and h are per-subject likelihood and hessians\n",
    "@time (betas, sigma, x, l, h) = em(df, subs, X, betas, sigma, model_asymmetry; emtol=1e-3, parallel=true, full=true, quiet=false);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Model Statistics \n",
    "(IAIC, IBIC, LOOCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## model selection/comparison/scoring\n",
    "\n",
    "# laplace approximation to the aggregate log marginal likelihood of the whole dataset\n",
    "# marginalized over the individual params\n",
    "\n",
    "aggll = lml(x,l,h)\n",
    "\n",
    "# to compare this between models you need to correct for the group-level free parameters\n",
    "# either aic or bic\n",
    "\n",
    "aggll_ibic = ibic(x,l,h,betas,sigma,nrow(df))\n",
    "aggll_iaic = iaic(x,l,h,betas,sigma)\n",
    "\n",
    "# or you can compute unbiased per subject marginal likelihoods via subject-level cross validation\n",
    "# you can do paired t tests on these between models\n",
    "# these are also appropriate for SPM_BMS etc\n",
    "\n",
    "# takes ages so comment in when want to run, otherwise just use IAIC above\n",
    "liks = loocv(df, subs, x, X, betas, sigma, model_asymmetry; emtol=1e-3, parallel=true, full=true)\n",
    "#aggll_loo = sum(liks)\n",
    "\n",
    "#println(\"\\n\\nraw nll:  $aggll\\nibic nll: $aggll_ibic\\niaic nll: $aggll_iaic\\nloo nll:  $aggll_loo\")\n",
    "#println(\"\\n\\nraw nll:  $aggll\\nibic nll: $aggll_ibic\\niaic nll:\")\n",
    "print(aggll_iaic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write loocv scores to csv file\n",
    "\n",
    "(if you have run loocv above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# put loocv scores into dataframe\n",
    "loocv_scores = DataFrame(sub = subs,\n",
    "liks = vec(liks));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save LOOCV to csv file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV.write(\"loocv_scores.csv\", DataFrame(loocv_scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### add to summary stats to LOOCV as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summary_stats = [summary_stats loocv_scores];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate and write p values, std error and covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard errors on the subject-level means, based on an asymptotic Gaussian approx \n",
    "# (these may be inflated esp for small n)\n",
    "(standarderrors, pvalues, covmtx) = emerrors(df,subs,x,X,h,betas,sigma,model_asymmetry);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_stats = DataFrame(stderror = vec(standarderrors),\n",
    "pvalues = vec(pvalues),\n",
    "covmtx_1 = vec(covmtx[:,1]),\n",
    "covmtx_2 = vec(covmtx[:,2]),\n",
    "covmtx_3 = vec(covmtx[:,3]),\n",
    "covmtx_4 = vec(covmtx[:,4]));\n",
    "\n",
    "# save model stats to csv file\n",
    "CSV.write(\"model_stats.csv\", DataFrame(model_stats));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(standarderrors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pvalues)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(covmtx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write per subject model parameters to csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put parameters into variable d\n",
    "d=x';\n",
    "\n",
    "# now put parameters into dataframe\n",
    "params = DataFrame(sub = subs,\n",
    "intercept = vec(d[:,1]), \n",
    "beta = vec(d[:,2]),\n",
    "learning_rate_reward_raw = vec(d[:,3]),\n",
    "learning_rate_reward_transformed = vec(0.5 .+ 0.5*erf.(d[:,3] / sqrt(2))),\n",
    "learning_rate_delay_raw = vec(d[:,4]),\n",
    "learning_rate_delay_transformed = vec(0.5 .+ 0.5*erf.(d[:,4] / sqrt(2))));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save parameters to csv file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV.write(\"subject_params.csv\", DataFrame(params))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save a copy with summary stats as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Delete non needed columns: exclude, exclude reason and duplicate sub column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "delete!(summary_stats, :sub);\n",
    "delete!(summary_stats, :exclude_reason);\n",
    "delete!(summary_stats, :exclude);\n",
    "delete!(summary_stats, :comment);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save summary stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = params[:,2:end]\n",
    "summary_stats = [summary_stats params]\n",
    "CSV.write(\"summary_stats.csv\", DataFrame(summary_stats))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test difference in learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# require contrast vector to test the difference in the learning parameters (from the covariance matrix)\n",
    "con_vec = [0, 0, 1, -1]\n",
    "\n",
    "#calculate standard error\n",
    "std_error = sqrt(con_vec'*covmtx*con_vec)\n",
    "\n",
    "#difference in (raw) learning rates\n",
    "learning_parm_diff = mean(params[:learning_rate_reward_raw]) - mean(params[:learning_rate_delay_raw])\n",
    "\n",
    "#now can derive tstatistic\n",
    "z_stat = learning_parm_diff/std_error\n",
    "\n",
    "#print these to output\n",
    "println(\"learning_parm_diff: \", learning_parm_diff);\n",
    "println(\"std_error: \", std_error);\n",
    "println(\"zstat: \", z_stat);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Generate by trial by trial values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Get best fit parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# if you already have best fit parameters saved, can read in here (rather than running model to find)\n",
    "params = CSV.read(\"subject_params.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Run model for each sub using best fit parameters\n",
    "Note: must rerun model with it set to return trial data (uncomment this)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# initialize parameter structures once again\n",
    "(df, subs, X, betas, sigma) = genVars(df, 4);\n",
    "\n",
    "# initalise this - will store all trial to trial parameters\n",
    "trial_data_compile = []\n",
    "\n",
    "# run model for each subject using best fit parameters\n",
    "for x = 1:length(subs)\n",
    "\n",
    "    # pull out optimal betas for subject - these are used in the model\n",
    "    # note: you want the unconverted learning score to be fed in\n",
    "    betas_sub = convert(Array, params[x, [:intercept, :beta, :learning_rate_reward_raw, :learning_rate_delay_raw]])\n",
    "    data_sub = df[df[:sub].==subs[x], :]\n",
    "    \n",
    "    # run model using these parameters - note must have commented in the model to return all of these variables (and not only -lik)\n",
    "    (minus_li, trial_data) = model_asymmetry(betas_sub, data_sub)\n",
    "    \n",
    "    if x==1\n",
    "        \n",
    "        trial_data_compile = trial_data\n",
    "        \n",
    "    else\n",
    "        \n",
    "        append!(trial_data_compile, trial_data)\n",
    "        \n",
    "    end\n",
    " \n",
    "end\n",
    "\n",
    "# check these are all the same sizes\n",
    "print(size(df))\n",
    "print(size(trial_data_compile))\n",
    "\n",
    "# print header of data compile\n",
    "head(trial_data_compile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Merge dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trial_data_compile = [df trial_data_compile];\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Delete sub column (this is a duplicate) and exclude columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "delete!(trial_data_compile, :sub);\n",
    "delete!(trial_data_compile, :exclude_reason);\n",
    "delete!(trial_data_compile, :exclude);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Save data to csv (in model folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CSV.write(\"trial_by_trial_values.csv\", DataFrame(trial_data_compile))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.7.0",
   "language": "julia",
   "name": "julia-0.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
