{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Neil Garrett, June 2018 **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Start up commands/load relevant functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# load required libraries\n",
    "using Distributed\n",
    "\n",
    "# # set everything up\n",
    "parallel = true # Run on multiple CPUs. If youhttp://localhost:8888/notebooks/Dropbox/Daw_Lab/PreySelection/v103/models/model_subjective1beta2lr_delayreward/model_subjective1beta2lr_delayreward.jl.ipynb# are having trouble, set parallel = false: easier to debug\n",
    "\n",
    "# this activates the multiprocessing threads\n",
    "if (parallel)\n",
    "\t# only run this once\n",
    "    addprocs(4)\n",
    "end\n",
    "\n",
    "# load required libraries\n",
    "@everywhere using DataFrames\n",
    "@everywhere using ForwardDiff\n",
    "@everywhere using PyCall\n",
    "@everywhere using Distributions\n",
    "@everywhere using PyPlot\n",
    "@everywhere using CSV\n",
    "@everywhere using SpecialFunctions\n",
    "@everywhere using SharedArrays\n",
    "@everywhere using LinearAlgebra\n",
    "\n",
    "@everywhere PyCall.@pyimport scipy.optimize as so\n",
    "\n",
    "# this is the code for the actual fitting routines\n",
    "@everywhere include(\"em.jl\")\n",
    "@everywhere include(\"common.jl\")\n",
    "@everywhere include(\"likfuns.jl\")\n",
    "\n",
    "# this is generates starting matricies for betas, sigmas etc to feed into model\n",
    "@everywhere include(\"genVars.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Data read and process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Read in trial by trial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#read in csv file of the data\n",
    "#trial by trial data: note will include force trials and missed responses\n",
    "df = readtable(\"/Users/neil/GitHubRepo/Projects/PreySelection/v103/data/trialdata_103_processed.csv\")\n",
    "\n",
    "#display header\n",
    "head(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Append data with the column \"sub\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#this is just a replica of the existing column sub_no but think em looks for \"sub\" specifically\n",
    "df[:sub] = df[:subj];\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Get rid of excluded subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = df[df[:exclude].==0,:];\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Convert approach avoid to 2s and 1s , missed as 0. Then convert to integers (necessary to use as an index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#convert approach_avoid to 1s (avoid) and 2s (approach)\n",
    "df[df[:approach_avoid].==1,:approach_avoid] = 2\n",
    "df[df[:approach_avoid].==-1,:approach_avoid] = 1\n",
    "\n",
    "index_NaN = find(isnan.(df[:approach_avoid]))\n",
    "df[index_NaN, :approach_avoid] = 0\n",
    "\n",
    "df[:approach_avoid] = convert(Vector{Integer}, df[:approach_avoid])\n",
    "\n",
    "head(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Read in summary stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "summary_stats = readtable(\"/Users/neil/GitHubRepo/Projects/PreySelection/v103/data/subdata_103.csv\")\n",
    "head(summary_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Get rid of excluded subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "summary_stats = summary_stats[summary_stats[:exclude].==0,:];\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Get rid of Mturk ID etc (first 3 columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "summary_stats = summary_stats[:,4:end];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MVT Model\n",
    "\n",
    "This model comprises: \n",
    "\n",
    "1. An intercept which reflects degree of bias to reject.\n",
    "\n",
    "2. A beta (termperature parameter) which controls sensitivity to the difference between the options (0 = pick 50/50. Higher it is, the more sensative subs are tothe different options (more step functionesque). <br>\n",
    "\n",
    "3. One learning rate\n",
    "\n",
    "Uses Q learned average to predict choice\n",
    "\n",
    "Initalise Qaverage in model at the arithmetic average over all subs over both sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@everywhere function model_MVT(params, data)\n",
    "     \n",
    "    #model parameters\n",
    "    intercept = params[1]\n",
    "    beta = params[2]\n",
    "    lr = 0.5\n",
    "    \n",
    "    w = 0.5 + 0.5 * erf(params[3] / sqrt(2))\n",
    "\n",
    "    #initalise various variables\n",
    "    delay_sum = zeros(typeof(beta),1)\n",
    "    reward_sum = zeros(typeof(beta),1)\n",
    "    Q_arithmetic = zeros(typeof(beta),1) # stores actual (arithmetic) average reward rate\n",
    "    opp_cost_arithmetic = zeros(typeof(beta),1) # stores actual (arithmetic) opp cost\n",
    "    Q_estimate = zeros(typeof(beta),1) .+ 8.22 # stores estimated global reward rate\n",
    "    opp_cost_estimate = zeros(typeof(beta),1) # stores estimated opp cost\n",
    "    \n",
    "    Qd = zeros(typeof(beta),2) # decision variable; 1st element is the opp cost of accepting (or value of rejecting), 2nd element is just the reward of the option (value of accepting)\n",
    "\n",
    "    lik = 0 #likelihood\n",
    "\n",
    "    #these store new trial by trial values (e.g. Q estimate on each trial etc.)\n",
    "    reward_sum_store = [];\n",
    "    delay_sum_store = [];\n",
    "    Q_arithmetic_store = [];\n",
    "    opp_cost_arithmetic_store  = []; \n",
    "    Q_estimate_store = [];\n",
    "    opp_cost_estimate_store = [];\n",
    "\n",
    "    #extract various variables from the dataframe\n",
    "    reward = data[:reward_percent]\n",
    "    delay = data[:delay_s]\n",
    "    force = data[:force_trial]  \n",
    "    missed = data[:missed] #missed responses \n",
    "    c = data[:approach_avoid] #choice\n",
    "        \n",
    "    for i = 1:length(c)\n",
    "        \n",
    "            # 2 seconds without reward on each trial regadless of accept/reject\n",
    "            delay_sum .+= 2;\n",
    "         \n",
    "            # calculate current (arithmetic) reward per second from number of seconds elapsed and reward accured\n",
    "            Q_arithmetic = reward_sum./delay_sum\n",
    "            opp_cost_arithmetic = Q_arithmetic*delay[i]\n",
    "           \n",
    "            PE = 0 - Q_estimate\n",
    "\n",
    "            # decrease estimate of global reward rate for encounter time (2seconds)\n",
    "            Q_estimate = (1-lr) * Q_estimate .+ 0\n",
    "        \n",
    "            lr = (1-w).*lr[1] + w.*PE^2\n",
    "        \n",
    "            PE = 0 - Q_estimate\n",
    "\n",
    "            Q_estimate = (1-lr) * Q_estimate .+ 0\n",
    "        \n",
    "            lr = (1-w).*lr[1] + w.*PE^2\n",
    "        \n",
    "            #calculate estimate of opportunity cost given estimate of reward rate and delay incurred by option \n",
    "            opp_cost_estimate = Q_estimate*delay[i]\n",
    "        \n",
    "            #add trial by trial values \n",
    "            append!(reward_sum_store, reward_sum)\n",
    "            append!(delay_sum_store, delay_sum)\n",
    "            append!(Q_arithmetic_store, Q_arithmetic)\n",
    "            append!(opp_cost_arithmetic_store, opp_cost_arithmetic)\n",
    "            append!(Q_estimate_store, Q_estimate)\n",
    "            append!(opp_cost_estimate_store, opp_cost_estimate)\n",
    "        \n",
    "            # if not a force trial predict choice based on current values\n",
    "            if ((force[i]<1) & (missed[i]<1))\n",
    "                        \n",
    "                # decision variable - the estimate of opportunity cost (\"reward\" of rejecting) versus \n",
    "                # reward of the current option (if accepted)\n",
    "                Qd = [intercept, 0] .+ [beta.*opp_cost_estimate[1], beta.*reward[i]]\n",
    "\n",
    "                # increment likelihood\n",
    "                lik += Qd[c[i]] - log(sum(exp.(Qd)))\n",
    "            \n",
    "            end\n",
    "            \n",
    "            #incur 8second time out for missed response\n",
    "            if (missed[i]==1)\n",
    "                \n",
    "                delay_sum .+= 8\n",
    "            \n",
    "                for j = 1:8\n",
    "                \n",
    "                    PE = 0 - Q_estimate\n",
    "\n",
    "                    Q_estimate = (1-lr) * Q_estimate .+ 0\n",
    "                \n",
    "                    lr = (1-w).*lr[1] + w.*PE^2\n",
    "\n",
    "                end\n",
    "            \n",
    "            end\n",
    "        \n",
    "            # regardless of whether a force trial or not, \n",
    "            # if accept the option, Q_estimate updates and there is a delay incurred\n",
    "            if ((c[i] == 2) & (missed[i]==0))\n",
    "                \n",
    "                delay_sum .+= delay[i]\n",
    "                reward_sum .+= reward[i]\n",
    "            \n",
    "                for j = 1:delay[i]\n",
    "                \n",
    "                    PE = 0 - Q_estimate\n",
    "\n",
    "                    Q_estimate = (1-lr) * Q_estimate .+ 0\n",
    "                \n",
    "                    lr = (1-w).*lr[1] + w.*PE^2\n",
    "                \n",
    "                end\n",
    "            \n",
    "                    PE = reward[i] - Q_estimate\n",
    "\n",
    "                    Q_estimate = (1-lr) * Q_estimate .+ lr*reward[i]\n",
    "            \n",
    "                    lr = (1-w).*lr[1] + w.*PE^2\n",
    "                \n",
    "            end\n",
    "    \n",
    "    end\n",
    "    \n",
    "    # compile trial by trial values here\n",
    "    trial_data = DataFrame(reward_sum = reward_sum_store,\n",
    "            delay_sum = delay_sum_store,\n",
    "            Q_arithmetic = Q_arithmetic_store,\n",
    "            opp_cost_arithmetic = opp_cost_arithmetic_store,\n",
    "            Q_estimate = Q_estimate_store,\n",
    "            opp_cost_estimate = opp_cost_estimate_store)\n",
    "    \n",
    "    # here if running em you can only return the likelihood\n",
    "    return -lik\n",
    "    \n",
    "    # but if you run in order to extract trials, subs etc then want to return this\n",
    "    #return (-lik, trial_data)\n",
    "    \n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter optimisiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model for one subject\n",
    "\n",
    "aids debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameter structures\n",
    "(df, subs, X, betas, sigma) = genVars(df, 3);\n",
    "\n",
    "# run model for sub 1\n",
    "model_MVT(betas,df[df[:sub].==subs[1],:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run em to get best fit parameters for each subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialized parameter structures (again)\n",
    "# note that some of the variables (e.g. betas, sigma) are entered and returned by em function \n",
    "(df, subs, X, betas, sigma) = genVars(df, 3);\n",
    "\n",
    "# run for full learner\n",
    "# x contains the parameters for each subject (note not the same as variable X)\n",
    "# l and h are per-subject likelihood and hessians\n",
    "@time (betas, sigma, x, l, h) = em(df, subs, X, betas, sigma, model_MVT; emtol=1e-3, parallel=true, full=true, quiet=false);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Model Statistics \n",
    "(IAIC, IBIC, LOOCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## model selection/comparison/scoring\n",
    "\n",
    "# laplace approximation to the aggregate log marginal likelihood of the whole dataset\n",
    "# marginalized over the individual params\n",
    "\n",
    "aggll = lml(x, l, h)\n",
    "\n",
    "# to compare this between models you need to correct for the group-level free parameters\n",
    "# either aic or bic\n",
    "\n",
    "aggll_ibic = ibic(x, l, h, betas, sigma, nrow(df))\n",
    "aggll_iaic = iaic(x, l, h, betas, sigma)\n",
    "\n",
    "# or you can compute unbiased per subject marginal likelihoods via subject-level cross validation\n",
    "# you can do paired t tests on these between models\n",
    "# these are also appropriate for SPM_BMS etc\n",
    "\n",
    "# takes ages so comment in when want to run, otherwise just use IAIC above\n",
    "liks = loocv(df, subs, x, X, betas, sigma, model_MVT; emtol=1e-3, parallel=true, full=true)\n",
    "#aggll_loo = sum(liks)\n",
    "\n",
    "#println(\"\\n\\nraw nll:  $aggll\\nibic nll: $aggll_ibic\\niaic nll: $aggll_iaic\\nloo nll:  $aggll_loo\")\n",
    "#println(\"\\n\\nraw nll:  $aggll\\nibic nll: $aggll_ibic\\niaic nll:\")\n",
    "print(aggll_iaic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write loocv scores to csv file\n",
    "\n",
    "(if you have run loocv above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# put loocv scores into dataframe\n",
    "loocv_scores = DataFrame(sub = subs,\n",
    "liks = vec(liks));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save LOOCV to csv file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV.write(\"loocv_scores.csv\", DataFrame(loocv_scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### add to summary stats to LOOCV as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summary_stats = [summary_stats loocv_scores];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate and write p values, std error and covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard errors on the subject-level means, based on an asymptotic Gaussian approx \n",
    "# (these may be inflated esp for small n)\n",
    "(standarderrors, pvalues, covmtx) = emerrors(df,subs,x,X,h,betas,sigma,model_MVT);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_stats = DataFrame(stderror = vec(standarderrors),\n",
    "pvalues = vec(pvalues),\n",
    "covmtx_1 = vec(covmtx[:,1]),\n",
    "covmtx_2 = vec(covmtx[:,2]),\n",
    "covmtx_3 = vec(covmtx[:,3]));\n",
    "\n",
    "# save model stats to csv file\n",
    "CSV.write(\"model_stats.csv\", DataFrame(model_stats));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(standarderrors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pvalues)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(covmtx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write per subject model parameters to csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put parameters into variable d\n",
    "d=x';\n",
    "\n",
    "# now put parameters into dataframe\n",
    "params = DataFrame(sub = subs,\n",
    "intercept = vec(d[:,1]), \n",
    "beta = vec(d[:,2]),\n",
    "learning_rate_raw = vec(d[:,3]),\n",
    "learning_rate_transformed = vec(0.5 .+ 0.5*erf.(d[:,3] / sqrt(2))));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save parameters to csv file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV.write(\"subject_params.csv\", DataFrame(params))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save a copy with summary stats as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Delete non needed columns: exclude, exclude reason and duplicate sub column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "delete!(summary_stats, :sub);\n",
    "delete!(summary_stats, :exclude_reason);\n",
    "delete!(summary_stats, :exclude);\n",
    "delete!(summary_stats, :comment);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save summary stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = params[:,2:end]\n",
    "summary_stats = [summary_stats params]\n",
    "CSV.write(\"summary_stats.csv\", DataFrame(summary_stats))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.7.0",
   "language": "julia",
   "name": "julia-0.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
