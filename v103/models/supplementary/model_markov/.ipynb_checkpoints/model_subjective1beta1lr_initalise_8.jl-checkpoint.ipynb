{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model subjective 1 beta 1 lr\n",
    "\n",
    "#### This model has one beta \n",
    "#### Also has an intercept\n",
    "\n",
    "#### one learning rate (no distinciton between appetative/aversive updates)\n",
    "\n",
    "#### Uses Q learned average to predict choice\n",
    "\n",
    "#### NJG, 14th November 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start up commands/load relevant functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set everything up\n",
    "parallel = true # Run on multiple CPUs. If you are having trouble, set parallel = false: easier to debug\n",
    "\n",
    "# this activates the multiprocessing threads\n",
    "if (parallel)\n",
    "\t# only run this once\n",
    "addprocs(4)\n",
    "end\n",
    "\n",
    "# load required libraries\n",
    "\n",
    "@everywhere using DataFrames\n",
    "@everywhere using ForwardDiff\n",
    "@everywhere using PyCall\n",
    "@everywhere using Distributions\n",
    "@everywhere using PyPlot\n",
    "\n",
    "@everywhere PyCall.@pyimport scipy.optimize as so\n",
    "\n",
    "# this is the code for the actual fitting routines\n",
    "@everywhere include(\"em.jl\")\n",
    "@everywhere include(\"common.jl\")\n",
    "@everywhere include(\"likfuns.jl\")\n",
    "\n",
    "# this is generates starting matricies for betas, sigmas etc to feed into model\n",
    "@everywhere include(\"genVars.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in task data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in csv file of the data\n",
    "df = readtable(\"/Users/neil/Dropbox/Daw_Lab/PreySelection/v103/data/subject_data_excluded_deleted.csv\")\n",
    "\n",
    "#note will include force trials and missed responses \n",
    "\n",
    "#display header\n",
    "head(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append data with the column \"sub\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is just a replica of the existing column sub_no but think em looks for \"sub\" specifically\n",
    "df[:sub] = df[:sub_no]\n",
    "head(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take out missed responses\n",
    "#### note: you will need to account for these at a later point as missing repsonses imposes a time delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exclude missed responses\n",
    "df = df[df[:missed].==0,:]\n",
    "head(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model: \"subjective\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@everywhere function model_subjective(params, data)\n",
    "        \n",
    "    intercept = params[1]\n",
    "    beta = params[2]\n",
    "    lr = 0.5 + 0.5*erf(params[3]/sqrt(2))\n",
    "    \n",
    "    #Q_av estimates reward in the environment\n",
    "    Q_av_per_time = zeros(typeof(beta),1)\n",
    "    delay_sum = zeros(typeof(beta),1)\n",
    "    reward_sum = zeros(typeof(beta),1)\n",
    "    opp_cost = zeros(typeof(beta),1)\n",
    "    \n",
    "    Q_estimate = zeros(typeof(beta),1)\n",
    "    opp_cost_estimate = zeros(typeof(beta),1)\n",
    "    \n",
    "    Qd = zeros(typeof(beta),2)\n",
    "\n",
    "    #initalise likelihood value\n",
    "    lik = 0\n",
    "    \n",
    "    reward = data[:reward]\n",
    "    delay = data[:delay]\n",
    "    force = data[:force_trial] \n",
    "    t = data[:trial_index] # trial \n",
    "    sub = data[:sub_no] # subject number\n",
    "    block = data[:block] # block\n",
    "    \n",
    "    c = data[:approach_avoid]\n",
    "    \n",
    "    #convert data to 1s (=avoid) and 2s (=approach); \n",
    "    # 1 (previously -1) is going to index choice to go with opportunity cost, \n",
    "    # 2 (previously +1) to go with the reward of the encountered option\n",
    "    c = c+1;\n",
    "    c_index_avoid = find(c.==0)\n",
    "    c[c_index_avoid] = 1\n",
    "        \n",
    "    #must convert floats (i.e. decimals) to integers in order to use as an index\n",
    "    c = convert(DataVector{Integer}, c)\n",
    "\n",
    "    \n",
    "    reward_sum_store = [];\n",
    "    delay_sum_store = [];\n",
    "    av_reward_store = [];\n",
    "    opp_cost_store  = []; \n",
    "    opp_cost_estimate_store = [];\n",
    "    Q_estimate_store = [];\n",
    "    \n",
    "    for i = 1:length(c)\n",
    "        \n",
    "            # 2 seconds without reward on each trial regadless of accept/reject\n",
    "            delay_sum += 2;\n",
    "         \n",
    "            # calculate current (arithmetic) reward per second from number of seconds elapsed and reward accured\n",
    "            Q_av_per_time = reward_sum./delay_sum\n",
    "            opp_cost = Q_av_per_time*delay[i]\n",
    "        \n",
    "            Q_estimate = (1-lr) * Q_estimate + 0\n",
    "            Q_estimate = (1-lr) * Q_estimate + 0\n",
    "        \n",
    "            opp_cost_estimate = Q_estimate*delay[i]\n",
    "\n",
    "            append!(reward_sum_store, reward_sum)\n",
    "            append!(delay_sum_store, delay_sum)\n",
    "            append!(av_reward_store, Q_av_per_time)\n",
    "            append!(opp_cost_store, opp_cost)\n",
    "            append!(opp_cost_estimate_store, opp_cost_estimate)\n",
    "            append!(Q_estimate_store, Q_estimate)\n",
    "        \n",
    "            # if not a force trial predict choice based on current values\n",
    "            if (force[i]<1)\n",
    "                        \n",
    "                # decision variable - the estimate of opportunity cost (\"reward\" of rejecting) versus \n",
    "                # reward of the current option (if accepted)\n",
    "                Qd = [intercept, 0] + [beta.*opp_cost_estimate[1], beta.*reward[i]]\n",
    "\n",
    "                # increment likelihood\n",
    "                lik += Qd[c[i]] - log(sum(exp.(Qd)))\n",
    "            \n",
    "            end\n",
    "        \n",
    "            # regardless of whether a force trial or not, \n",
    "            # if accept the option, Qreward updates and there is a longer period of delay\n",
    "            if (c[i] == 2)\n",
    "                \n",
    "                delay_sum += delay[i]\n",
    "                reward_sum += reward[i]\n",
    "            \n",
    "                for i = 1:length(delay[i])\n",
    "                \n",
    "                    Q_estimate = (1-lr) * Q_estimate + 0\n",
    "                \n",
    "                end\n",
    "            \n",
    "                    Q_estimate = (1-lr) * Q_estimate + lr*reward[i]\n",
    "                \n",
    "            end\n",
    "    \n",
    "    end\n",
    "    \n",
    "     trial_data = DataFrame([sub,\n",
    "            block,\n",
    "            t,\n",
    "            force,\n",
    "            reward,\n",
    "            delay,\n",
    "            c,\n",
    "            reward_sum_store,\n",
    "            delay_sum_store,\n",
    "            av_reward_store,\n",
    "            opp_cost_store,\n",
    "            Q_estimate_store,\n",
    "            opp_cost_estimate_store])\n",
    "    \n",
    "    # detail names of variables - frustrating this is neccesary\n",
    "    names!(trial_data,[:sub,\n",
    "            :block,\n",
    "            :trial,\n",
    "            :force,\n",
    "            :reward,\n",
    "            :delay,\n",
    "            :choice,\n",
    "            :reward_sum,\n",
    "            :delay_sum,\n",
    "            :avreward_arithmetic,\n",
    "            :opp_cost_arithmetic,\n",
    "            :avreward_estimate,\n",
    "            :opp_cost_estimate])\n",
    "    \n",
    "    \n",
    "    # here if running em you can only return the likelihood\n",
    "    return -lik\n",
    "    \n",
    "    # but if you run in order to extract trials, subs etc then want to return this\n",
    "    #return (-lik, trial_data)\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model for one subject\n",
    "\n",
    "##### aids debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameter structures\n",
    "(df, subs, X, betas, sigma) = genVars(df, 3);\n",
    "\n",
    "# run model for sub 1\n",
    "model_subjective(betas,df[df[:sub].==subs[1],:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run em to get best fit parameters for each subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialized parameter structures (again)\n",
    "# note that some of the variables (e.g. betas, sigma) are entered and returned by em function \n",
    "(df, subs, X, betas, sigma) = genVars(df, 3);\n",
    "\n",
    "# run for full learner\n",
    "# x contains the parameters for each subject (note not the same as variable X)\n",
    "# l and h are per-subject likelihood and hessians\n",
    "@time (betas, sigma, x, l, h) = em(df, subs, X, betas, sigma, model_subjective; emtol=1e-3, parallel=true, full=true, quiet=false);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Model Statistics \n",
    "#### (IAIC, LOOCV, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## model selection/comparison/scoring\n",
    "\n",
    "# laplace approximation to the aggregate log marginal likelihood of the whole dataset\n",
    "# marginalized over the individual params\n",
    "\n",
    "aggll = lml(x,l,h)\n",
    "\n",
    "# to compare this between models you need to correct for the group-level free parameters\n",
    "# either aic or bic\n",
    "\n",
    "aggll_ibic = ibic(x,l,h,betas,sigma,nrow(df))\n",
    "aggll_iaic = iaic(x,l,h,betas,sigma)\n",
    "\n",
    "# or you can compute unbiased per subject marginal likelihoods via subject-level cross validation\n",
    "# you can do paired t tests on these between models\n",
    "# these are also appropriate for SPM_BMS etc\n",
    "\n",
    "# takes ages so comment in when want to run, otherwise just use IAIC above\n",
    "\n",
    "#liks = loocv(df, subs, x, X, betas, sigma, model_subjective; emtol=1e-3, parallel=true, full=true)\n",
    "#aggll_loo = sum(liks)\n",
    "\n",
    "#println(\"\\n\\nraw nll:  $aggll\\nibic nll: $aggll_ibic\\niaic nll: $aggll_iaic\\nloo nll:  $aggll_loo\")\n",
    "#println(\"\\n\\nraw nll:  $aggll\\nibic nll: $aggll_ibic\\niaic nll:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write loocv scores to csv file\n",
    "\n",
    "#### (if you have run this part above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put loocv scores into dataframe\n",
    " loocv_scores = DataFrame(sub = subs,\n",
    " liks = vec(liks));\n",
    "\n",
    "# save loocv scores to csv file\n",
    " writetable(\"loocv_scores.csv\", DataFrame(loocv_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write per subject model parameters to csv file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put parameters into variable d\n",
    "d=x';\n",
    "\n",
    "# now put parameters into dataframe\n",
    "params = DataFrame(sub = subs,\n",
    "intercept = vec(d[:,1]), \n",
    "beta = vec(d[:,2]),\n",
    "learning_rate_raw = vec(d[:,3]),\n",
    "learning_rate_transformed = vec(0.5 + 0.5*erf.(d[:,3] / sqrt(2))));\n",
    "\n",
    "# save parameters to csv file\n",
    "writetable(\"subject_params.csv\", DataFrame(params))\n",
    "\n",
    "#or: CSV.write(\"subject_params_full_learner.csv\",params_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now run  model with these parameters for each subject to get trial by trial Q values\n",
    "##### Note: must rerun model with it set to return trial data (uncomment this)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if you already have best fit parameters saved, can read in here (rather than running model to find)\n",
    "params = readtable(\"subject_params.csv\")\n",
    "head(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run model for each sub using best fit parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize parameter structures once again\n",
    "(df, subs, X, betas, sigma) = genVars(df, 3);\n",
    "\n",
    "# initalise this - will store all trial to trial parameters\n",
    "trial_data_compile = []\n",
    "\n",
    "# run model for each subject using best fit parameters\n",
    "for x = 1:length(subs)\n",
    "\n",
    "    # pull out optimal betas for subject - these are used in the model\n",
    "    # note: you want the unconverted learning score to be fed in\n",
    "    betas_sub = Array(params[x, [:intercept, :beta, :learning_rate_raw]])\n",
    "    data_sub = df[df[:sub].==subs[x], :]\n",
    "    \n",
    "    # run model using these parameters - note must have commented in the model to return all of these variables (and not only -lik)\n",
    "    (minus_li, trial_data) = model_subjective(betas_sub, data_sub)\n",
    "    \n",
    "    if x==1\n",
    "        \n",
    "        trial_data_compile = trial_data\n",
    "        \n",
    "    else\n",
    "        \n",
    "        append!(trial_data_compile, trial_data)\n",
    "        \n",
    "    end\n",
    " \n",
    "end\n",
    "\n",
    "# check these are all the same sizes\n",
    "print(size(df))\n",
    "print(size(trial_data_compile))\n",
    "\n",
    "# print header of data compile\n",
    "head(trial_data_compile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate probabilities of choosing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ProbAccept_ALL = []\n",
    "ProbReject_ALL = [] \n",
    "ProbAccept_minus_ProbReject_ALL = []\n",
    "\n",
    "for x = 1:length(subs)\n",
    "\n",
    "    current_sub = subs[x];\n",
    "    \n",
    "    # pull out optimal betas for subject - these are used in the model\n",
    "    # note: you want the unconverted learning score to be fed in\n",
    "    betas_sub = Array(params[x, [:intercept, :beta]])\n",
    "    \n",
    "    intercept = betas_sub[1] \n",
    "    beta = betas_sub[2]\n",
    "            \n",
    "    subset_data = trial_data_compile[trial_data_compile[:sub].==subs[x], :]\n",
    "    \n",
    "    n_trials = size(subset_data); n_trials = n_trials[1]\n",
    "\n",
    "    ProbAccept = zeros(n_trials)\n",
    "    ProbReject = zeros(n_trials)\n",
    "    ProbAccept_minus_ProbReject = zeros(n_trials)\n",
    "    \n",
    "    accept_value = subset_data[:reward]\n",
    "    reject_value = subset_data[:opp_cost_estimate]\n",
    "    choices = subset_data[:choice]\n",
    "    \n",
    "    for t = 1:n_trials\n",
    "                       \n",
    "        ProbAccept[t] = exp(0 + beta*accept_value[t])/(exp(0 + beta*accept_value[t]) + exp(intercept + beta*reject_value[t])) \n",
    "        ProbReject[t] = 1 - ProbAccept[t];\n",
    "        ProbAccept_minus_ProbReject[t] = ProbAccept[t] - ProbReject[t];\n",
    "         \n",
    "    end\n",
    "\n",
    "    ProbAccept_ALL = [ProbAccept_ALL; ProbAccept]\n",
    "    ProbReject_ALL = [ProbReject_ALL; ProbReject]\n",
    "    ProbAccept_minus_ProbReject_ALL = [ProbAccept_minus_ProbReject_ALL; ProbAccept_minus_ProbReject]\n",
    "    \n",
    "end\n",
    "\n",
    "#Now bung into data frame and merge with rest\n",
    "Q_probs = DataFrame([ProbAccept_ALL, \n",
    "        ProbReject_ALL, \n",
    "        ProbAccept_minus_ProbReject_ALL]) \n",
    "\n",
    "#annoying - must be a better way to do this\n",
    "names!(Q_probs, [:ProbAccept, \n",
    "        :ProbReject, \n",
    "        :ProbAccept_minus_ProbReject])\n",
    "\n",
    "# now merge the two dataframes together (note this overwrites previous full compile)\n",
    "trial_data_compile = hcat(trial_data_compile, Q_probs); #could also do just: [full_Q_compile Q_probs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data to csv in model folder\n",
    "##### NOTE: after this note you must save as an xlsx file to run in matlab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "writetable(\"trial_by_trial_values.csv\", DataFrame(trial_data_compile))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "println(\"intercept min: \", minimum(params[:intercept]))\n",
    "println(\"intercept max: \", maximum(params[:intercept]))\n",
    "println(\"beta min: \", minimum(params[:beta]))\n",
    "println(\"beta max: \", maximum(params[:beta]))\n",
    "println(\"lr min: \", minimum(params[:learning_rate_transformed]))\n",
    "println(\"lr max: \", maximum(params[:learning_rate_transformed]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = [1, 2, 3]\n",
    "\n",
    "my_xticks = [\"intercept\",\"beta\", \"lr\"]\n",
    "\n",
    "y=[mean(params[:intercept]), mean(params[:beta]), mean(params[:learning_rate_transformed])]\n",
    "\n",
    "PyPlot.plt[:xticks](x, my_xticks)\n",
    "PyPlot.plt[:bar](x,y,color=\"#0f87bf\",align=\"center\",alpha=0.4)\n",
    "title(\"average parameter values\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### NOTE: intercept in the model is put on the value of rejecting: hence a negative value suggests a bias away from rejecting (the value of rejecting is devalued)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PyPlot.plt[:hist](params[:intercept],10)\n",
    "title(\"Histrogram of intercept parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PyPlot.plt[:hist](params[:beta],10)\n",
    "title(\"Histrogram of beta value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PyPlot.plt[:hist](params[:learning_rate_transformed],10)\n",
    "title(\"Histrogram of learning parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PyPlot.plt[:scatter](params[:beta],params[:learning_rate_transformed])\n",
    "title(\"learning parameters: beta vs lr\")\n",
    "xlabel(\"beta\")\n",
    "ylabel(\"learning_rate\")\n",
    "\n",
    "println(\"correlation: \", cor(params[:beta],params[:learning_rate_transformed]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for x = 1:length(subs)\n",
    "\n",
    "    current_sub = subs[x];\n",
    "    \n",
    "    subset_data_all = trial_data_compile[trial_data_compile[:sub].==current_sub, :]\n",
    "    \n",
    "    #subset_data_b1 = \n",
    "    #subset_data_b2  = \n",
    "    \n",
    "    X = subset_data_all[:trial]\n",
    "    Y = subset_data_all[:opp_cost_arithmetic]\n",
    "    \n",
    "    subplot(7,7,x)\n",
    "\n",
    "    PyPlot.plt[:scatter](X,Y,s=0.5)\n",
    "            \n",
    "end\n",
    "\n",
    "suptitle(\"Arithmetic opportunity cost over time for each sub\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### opp cost fluctuates trial by trial a lot depending on the options (their delay) as well as the average reward rate \n",
    "#### hence why it looks like two lines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for x = 1:length(subs)\n",
    "\n",
    "    current_sub = subs[x];\n",
    "    \n",
    "    subset_data_all = trial_data_compile[trial_data_compile[:sub].==current_sub, :]\n",
    "    \n",
    "    \n",
    "    X = subset_data_all[:trial]\n",
    "    Y = subset_data_all[:opp_cost_estimate]\n",
    "    \n",
    "    subplot(7,7,x)\n",
    "\n",
    "    PyPlot.plt[:scatter](X,Y,s=0.5)\n",
    "            \n",
    "end\n",
    "\n",
    "suptitle(\"Estimated opportunity cost over time for each sub\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for x = 1:length(subs)\n",
    "\n",
    "    current_sub = subs[x];\n",
    "    \n",
    "    subset_data_all = trial_data_compile[trial_data_compile[:sub].==current_sub, :]\n",
    "    \n",
    "    #subset_data_b1 = \n",
    "    #subset_data_b2  = \n",
    "    \n",
    "    X = subset_data_all[:trial]\n",
    "    Y = subset_data_all[:avreward_arithmetic]\n",
    "    \n",
    "    subplot(7,7,x)\n",
    "\n",
    "    PyPlot.plt[:scatter](X,Y,s=0.5)\n",
    "            \n",
    "end\n",
    "\n",
    "suptitle(\"Arithmetic average reward rate over time for each sub\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for x = 1:length(subs)\n",
    "\n",
    "    current_sub = subs[x];\n",
    "    \n",
    "    subset_data_all = trial_data_compile[trial_data_compile[:sub].==current_sub, :]\n",
    "    \n",
    "    #subset_data_b1 = \n",
    "    #subset_data_b2  = \n",
    "    \n",
    "    X = subset_data_all[:trial]\n",
    "    Y = subset_data_all[:avreward_estimate]\n",
    "    \n",
    "    subplot(7,7,x)\n",
    "\n",
    "    PyPlot.plt[:scatter](X,Y,s=0.5)\n",
    "            \n",
    "end\n",
    "\n",
    "suptitle(\"Estimated average reward rate over time for each sub\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.1",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
