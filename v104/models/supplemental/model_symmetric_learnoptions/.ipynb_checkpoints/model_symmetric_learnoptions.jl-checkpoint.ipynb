{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Neil Garrett, June 2018 **\n",
    "uses julia EM model fitting by Nathaniel Daw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start up commands/load relevant functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parallel = true # Run on multiple CPUs. If you are having trouble, set parallel = false: easier to debug\n",
    "full = false    # Maintain full covariance matrix (vs a diagional one) at the group level\n",
    "emtol = 1e-3    # stopping condition (relative change) for EM\n",
    "\n",
    "using Distributed\n",
    "if (parallel)\n",
    "\t# only run this once\n",
    "\taddprocs()\n",
    "end\n",
    "\n",
    "# this loads the packages needed -- the @everywhere makes sure they \n",
    "# available on all CPUs \n",
    "\n",
    "@everywhere using DataFrames\n",
    "@everywhere using SharedArrays\n",
    "@everywhere using ForwardDiff\n",
    "@everywhere using Optim\n",
    "@everywhere using LinearAlgebra       # for tr, diagonal\n",
    "@everywhere using StatsFuns           # logsumexp\n",
    "@everywhere using SpecialFunctions    # for erf\n",
    "@everywhere using Statistics          # for mean\n",
    "@everywhere using Distributions\n",
    "@everywhere using GLM\n",
    "@everywhere using CSV #for reading/writing csv files\n",
    "\n",
    "# change this to where you keep the Daw's latest em code\n",
    "@everywhere directory = \"/Users/neil/GitHubRepo/Projects/PreySelection/em\"\n",
    "\n",
    "#load in functions including em\n",
    "@everywhere include(\"$directory/em.jl\");\n",
    "@everywhere include(\"$directory/common.jl\");\n",
    "@everywhere include(\"$directory/likfuns.jl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data read and process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in trial by trial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#read in csv file of the data\n",
    "#trial by trial data: note will include force trials and missed responses\n",
    "df = CSV.read(\"/Users/neil/GitHubRepo/Projects/PreySelection/v104/data/trialdata_104_processed.csv\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get rid of excluded subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = df[df[:exclude].==0,:];\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert approach avoid to 2s and 1s , missed as 0. Then convert to integers (necessary to use as an index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# convert approach_avoid to 1s (avoid) and 2s (approach)\n",
    "df[df[:approach_avoid].==1,:approach_avoid] = 2\n",
    "df[df[:approach_avoid].==-1,:approach_avoid] = 1\n",
    "\n",
    "# put 0 for missed responses\n",
    "index_NaN = findall(isnan.(df[:approach_avoid]))\n",
    "df[index_NaN, :approach_avoid] = 0\n",
    "\n",
    "df[:approach_avoid] = convert(Vector{Integer}, df[:approach_avoid])\n",
    "\n",
    "first(df, 6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Symmetric Model\n",
    "\n",
    "This model comprises: \n",
    "\n",
    "1. An intercept which reflects degree of bias to reject.\n",
    "\n",
    "2. A beta (termperature parameter) which controls sensitivity to the difference between the options (0 = pick 50/50. Higher it is, the more sensative subs are tothe different options (more step functionesque). <br>\n",
    "\n",
    "3. One learning rate\n",
    "\n",
    "Uses Q learned average to predict choice\n",
    "\n",
    "Initalise Qaverage in model at the arithmetic average over all subs over both sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@everywhere function model_symmetric(params, data)\n",
    "     \n",
    "      \n",
    "    #model parameters\n",
    "    intercept = params[1]\n",
    "    beta = params[2]\n",
    "    lr_environment = 0.5 .+ 0.5.*erf(params[3]/sqrt(2))\n",
    "    lr_options = 0.5 .+ 0.5.*erf(params[4]/sqrt(2))\n",
    "    \n",
    "    #initalise various variables\n",
    "    Q_estimate = zeros(typeof(beta),1) .+ 7.77 # stores estimated global reward rate\n",
    "    opp_cost_estimate = zeros(typeof(beta),1) # stores estimated opp cost\n",
    "    \n",
    "    Qd = zeros(typeof(beta),2) # decision variable; 1st element is the opp cost of accepting (or value of rejecting), 2nd element is just the reward of the option (value of accepting)\n",
    "\n",
    "    # maintains estimate of the reward for each option (options indexed by rank: 1-4). \n",
    "    Q_options_reward = zeros(typeof(beta), 4)\n",
    "    Q_options_delay = zeros(typeof(beta), 4)\n",
    "    \n",
    "    lik = 0 #likelihood\n",
    "\n",
    "    #extract various variables from the dataframe\n",
    "    reward = data[:reward_percent]\n",
    "    delay = data[:delay_s]\n",
    "    force = data[:force_trial]  \n",
    "    missed = data[:missed] #missed responses \n",
    "    c = data[:approach_avoid] #choice\n",
    "    option_rank = data[:stim_rank] #option rank \n",
    "\n",
    "    for i = 1:length(c)\n",
    "        \n",
    "            #option presented in current trial\n",
    "            option_index = option_rank[i]\n",
    "           \n",
    "            # decrease estimate of global reward rate for encounter time (2seconds)\n",
    "            Q_estimate = (1-lr_environment)*Q_estimate .+ 0\n",
    "            Q_estimate = (1-lr_environment)*Q_estimate .+ 0\n",
    "        \n",
    "            #calculate estimate of opportunity cost given estimate of reward rate and delay incurred by option \n",
    "            opp_cost_estimate = Q_estimate*Q_options_delay[option_index]\n",
    "        \n",
    "            # if not a force trial predict choice based on current values\n",
    "            if ((force[i]<1) & (missed[i]<1))\n",
    "                        \n",
    "                # decision variable - the estimate of opportunity cost (\"reward\" of rejecting) versus \n",
    "                # reward of the current option (if accepted)\n",
    "                Qd = [intercept, 0] .+ [beta.*opp_cost_estimate[1], beta.*Q_options_reward[option_index]]\n",
    "\n",
    "                # increment likelihood\n",
    "                lik += Qd[c[i]] - log(sum(exp.(Qd)))\n",
    "            \n",
    "            end\n",
    "            \n",
    "            #incur 8second time out for missed response\n",
    "            if (missed[i]==1)\n",
    "                            \n",
    "                for j = 1:8\n",
    "                \n",
    "                     Q_estimate = (1-lr_environment)*Q_estimate .+ 0\n",
    "\n",
    "                end\n",
    "            \n",
    "            end\n",
    "        \n",
    "            # regardless of whether a force trial or not, \n",
    "            # if accept the option, Q_estimate updates and there is a delay incurred\n",
    "            if ((c[i] == 2) & (missed[i]==0))\n",
    "            \n",
    "                for j = 1:delay[i]\n",
    "                \n",
    "                    Q_estimate = (1-lr_environment) * Q_estimate .+ 0\n",
    "                \n",
    "                end\n",
    "            \n",
    "                Q_estimate = (1-lr_environment) * Q_estimate .+ lr_environment*reward[i]\n",
    "            \n",
    "                Q_options_reward[option_index] = (1-lr_options)*Q_options_reward[option_index] .+ lr_options*reward[i]\n",
    "                Q_options_delay[option_index] = (1-lr_options)*Q_options_delay[option_index] .+ lr_options*delay[i]\n",
    "                \n",
    "            end\n",
    "    \n",
    "    end\n",
    "    \n",
    "    # here if running em you can only return the likelihood\n",
    "    return -lik\n",
    "    \n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter optimisiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setup variables for em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#store list of actual subject numbers (in subj)\n",
    "subs = unique(df[:subj])\n",
    "\n",
    "#put in a new column called \"sub\" which is identical to subj - em looks for this\n",
    "df[:sub] = df[:subj];\n",
    "\n",
    "NS = length(subs)\n",
    "X = ones(NS)\n",
    "betas = [0. 0. 0. 0.]\n",
    "sigma = [1., 1., 1., 1.];\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run em to get best fit parameters for each subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# run em\n",
    "# x contains the parameters for each subject (note not the same as variable X)\n",
    "# l and h are per-subject likelihood and hessians\n",
    "(betas, sigma, x, l, h) = em(df, subs, X, betas, sigma, model_symmetric; emtol=emtol, parallel=parallel, full=full);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Model Statistics \n",
    "(LOOCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#compute unbiased per subject marginal likelihoods via cross validation.\n",
    "liks = loocv(df, subs, x, X, betas, sigma, model_symmetric; emtol=emtol, parallel=parallel, full=full)\n",
    "\n",
    "print(sum(liks))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write loocv scores to csv file and save\n",
    "\n",
    "(if you have run loocv above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#put loocv scores into dataframe\n",
    "loocv_scores = DataFrame(sub = subs,\n",
    "liks = vec(liks));\n",
    "\n",
    "CSV.write(\"loocv_scores.csv\", DataFrame(loocv_scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate and write p values, std error and covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# standard errors on the subject-level means, based on an asymptotic Gaussian approx \n",
    "# (these may be inflated esp for small n)\n",
    "(standarderrors, pvalues, covmtx) = emerrors(df, subs, x, X, h, betas, sigma, model_symmetric);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_stats = DataFrame(stderror = vec(standarderrors),\n",
    "pvalues = vec(pvalues),\n",
    "covmtx_1 = vec(covmtx[:,1]),\n",
    "covmtx_2 = vec(covmtx[:,2]),\n",
    "covmtx_3 = vec(covmtx[:,3]),\n",
    "covmtx_4 = vec(covmtx[:,4]));\n",
    "\n",
    "# save model stats to csv file\n",
    "CSV.write(\"model_stats.csv\", DataFrame(model_stats));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write per subject model parameters to csv files and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# put parameters into variable d\n",
    "d=x;\n",
    "\n",
    "# now put parameters into dataframe\n",
    "params = DataFrame(sub = subs,\n",
    "intercept = vec(d[:,1]), \n",
    "beta = vec(d[:,2]),\n",
    "learning_rate_environment_raw = vec(d[:,3]),\n",
    "lr_options_raw = vec(d[:,4]));\n",
    "\n",
    "CSV.write(\"subject_params.csv\", DataFrame(params))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.7.0",
   "language": "julia",
   "name": "julia-0.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
